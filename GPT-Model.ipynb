{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fd631be",
   "metadata": {},
   "source": [
    "# This is my first expirementational code with a GPT model\n",
    " I am following a tutorial made by Andrej Karpathy and plan on adding my own stuff throughout the process to improve my gained knowledge and understanding.\n",
    "\n",
    "\n",
    "### First we start off by downloading this tinyshakespeare text.\n",
    " This wil be our starting data set for the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bf2ba1-5490-4fc8-91d9-d8359b4efc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb16f6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb4fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab909cef",
   "metadata": {},
   "source": [
    "Here we are defining out vocabulary set. We do this by getting rid of the duplicates, then sorting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c41009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a74f62",
   "metadata": {},
   "source": [
    "Now we are doing the encoding and decoding. This is done in the following way:\n",
    "we create lookup tables (dictionaries) from number to characters and vice-versa.\n",
    "Then when encoding, we find the index of the characters in the given string, and return the array of indices.\n",
    "As expected, for decoding, we get the collection of characters and join them together according to the given array of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fab74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6511822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_hello = encode(\"hello\")\n",
    "decoded_hello = decode(encoded_hello)\n",
    "print(encoded_hello)\n",
    "print(decoded_hello)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c493bf7f",
   "metadata": {},
   "source": [
    "In our case, we are using a very simple technique/method. This has its tradeoffs. This simplicity will make it easier to get a \"finished\" product, and results in easier and less complex encode and decode functions. However, there are alternatives that use much larger vocabularies (using subphrases - I believe) that results in a smaller result when encoding. I am assuming that this results in an improvement to the speed of the model at the trade off of complexity and a larger vocabulary size (storage size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd661ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818fe888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e79a64",
   "metadata": {},
   "source": [
    "Now, we are encoding all the text in the little shakespeare dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57048fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74552177",
   "metadata": {},
   "source": [
    "Let's split into a training and validation data set now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b4779be",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(0.85*len(data))\n",
    "\n",
    "train_data = data[:split]\n",
    "val_data = data[split:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db6f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8\n",
    "\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310a640c",
   "metadata": {},
   "source": [
    "the point of doing it like this is: the model gets used to understanding the sequence of characters etc.\n",
    "But most importantly, for generation, it is trained to start off with just 1 character of context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b9ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range( block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'when input is {context} target: {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b280238",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337) #pointless* - just to get same numbers as the video\n",
    "\n",
    "batch_size = 4 # this is how many sequences we will process in parallel\n",
    "\n",
    "block_size = 8 # again, its how much the maximum length of context will be\n",
    "\n",
    "def get_batch(split: str) -> tuple:\n",
    "    data = train_data if split==\"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # Create a tensor by stacking slices of data, each slice is of length block_size\n",
    "    x = torch.stack([\n",
    "        data[i:i+block_size]\n",
    "        for i in ix])\n",
    "    # Create a tensor by stacking slices of data, each slice is of length block_size\n",
    "    # but shifted by 1 position to the right to serve as the target output\n",
    "    y = torch.stack([\n",
    "        data[i+1: i+block_size+1]\n",
    "        for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15e787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, : t + 1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context.tolist()}  | target: {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed11cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLM(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        # creating a 65 x 65 table\n",
    "\n",
    "    def forward(self, idx, targets= None):\n",
    "\n",
    "        # so what we're doing is:\n",
    "        # we pass in the index, and get that row. I assume this will be the things that usually follow the passed in encoded value?\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C) B = batch | T = Time | C = vocab\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets) # this figures out how well we are predicting the next character\n",
    "        # targets is passed in (training data so we know expected output)\n",
    "        # this however doesn't work because: targets currently is (B x T) x C but torch wants it in the form BxCxT\n",
    "        # so we need to reshape logits\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens) -> int:\n",
    "        \"\"\" to summarize:\n",
    "\n",
    "        this takes the passed indices, then generates tokens (max_new_tokens is the number of tokens we generate)\n",
    "        and return the passed in indices with the generated ones appended on it.\n",
    "        Basicaly, we give it \"he\" and the number 3, then expect \"hello\" (as an example)\n",
    "        This is done using probabilities\n",
    "\n",
    "        T is removed as we only care about going in sequence rather than moving through time arbitrarily.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # IDX is (B , T) array of indicies (current context)\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            # focus on last time step\n",
    "            logits = logits[:, -1, :] # gives it in the form (B, C)\n",
    "            # softamx to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # Get a smaple from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # add sampled to the current sequence (kind of like autocomplete?)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "m = BigramLM(vocab_size)\n",
    "out, loss = m(xb, yb)\n",
    "print(out.shape)\n",
    "print(loss)\n",
    "# expected loss = -ln(1/vocab_size)\n",
    "idx = torch.zeros((1, 1), dtype=torch.long) # we pass in the zero character in our vocab (aka new line) This helps with automated text generation\n",
    "# if we pass in something else, it will work more like autocorrect rather than text generation\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))\n",
    "# so we are generating 100 new tokens, with the new line passed in. Then turning it into a list and decoding it from the encoded vocab calues\n",
    "# right now, this is completely random so we need to train it\n",
    "# right now the generate function is a bigram model, however, we still pass in all the tokens, this is so we don't have to change it later when its better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27d24058",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(),lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b21ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "for steps in range(50000):\n",
    "\n",
    "    xb, yb= get_batch('train')\n",
    "\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53d9e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode(m.generate(idx, max_new_tokens=500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fd02de",
   "metadata": {},
   "source": [
    "Mathematical Trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43060493",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "B,T,C = 4,8,2\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93364d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens should only talk to tokens in the past\n",
    "# easiest way for tokens to communicate is to average over all the tokens in the past\n",
    "xbow = torch.zeros((B,T,C)) # x Bag of words\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xbow[b,t] = x[b,:t+1].mean(dim=0)\n",
    "xbow.shape\n",
    "\n",
    "# this is a very simple way to do it, but it doesn't take into account the context of the tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84f15d7",
   "metadata": {},
   "source": [
    "Here we use batched matrix multiplication in order to get the weighted sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ed033a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tril(torch.ones(T, T))\n",
    "weights = weights / weights.sum(dim=1, keepdim=True)\n",
    "xbow2 = weights @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cdb2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, dim=1, keepdim=True) # to summarize, this gives us a matrix, where each row sums to 1.\n",
    "# the number of non-zero elements in each row is the number of tokens in the past that the token in the row talks to.\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "c = a @ b\n",
    "print(c)\n",
    "print(c.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041b7bcf",
   "metadata": {},
   "source": [
    "this is the best way as the weights begin with 0. This allows for easier changing of the 'affinities' so it helps us decide and alter to get suitable weights for the tokens rather than setting explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bbdd9686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "weights = torch.zeros((T,T))\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "xbow3 = weights @ x\n",
    "torch.allclose(xbow2, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "27284916",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "\n",
    "weights = q @ k.transpose(-2, -1) # B, T, 16 @ B, 16, T = B, T, T\n",
    "\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#weights = torch.zeros((T,T))\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "v = value(x)\n",
    "out = weights @ v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddf2346",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        xmean = x.mean(dim=1)\n",
    "        xvar = x.var(dim=1)\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe21e91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
